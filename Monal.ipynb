{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12c668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini API client\n",
    "from google import genai\n",
    "\n",
    "# Standard libraries\n",
    "import json          # Handle JSON data\n",
    "import base64        # Encode/decode image data\n",
    "import io            # Work with in-memory byte streams\n",
    "import ast           # Safely evaluate Python literals\n",
    "import re            # Clean and process text\n",
    "import os            # File and directory operations\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# UI framework\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "873511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini client using API key from environment variables (Google AI Studio)\n",
    "\n",
    "client = genai.Client(api_key=os.getenv('api_key'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a323ace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python offers several straightforward ways to reverse a string. The most Pythonic and commonly used method is string slicing.\n",
      "\n",
      "Here are a few methods, from the most Pythonic to more explicit loop-based approaches:\n",
      "\n",
      "---\n",
      "\n",
      "### Method 1: Using Slicing `[::-1]` (Most Pythonic and Recommended)\n",
      "\n",
      "This is the simplest and most idiomatic way to reverse a string in Python. It creates a reversed copy of the string.\n",
      "\n",
      "```python\n",
      "def reverse_string_slice(s):\n",
      "  \"\"\"Reverses a string using slicing.\"\"\"\n",
      "  return s[::-1]\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"Slice Method:\")\n",
      "print(f\"'hello' reversed is: {reverse_string_slice('hello')}\")         # Output: olleh\n",
      "print(f\"'Python' reversed is: {reverse_string_slice('Python')}\")       # Output: nohtyP\n",
      "print(f\"'' reversed is: {reverse_string_slice('')}\")                   # Output:\n",
      "print(f\"'a' reversed is: {reverse_string_slice('a')}\")                 # Output: a\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "*   `s` is your string.\n",
      "*   `[::-1]` creates a reversed copy. The first two `:` indicate a full slice (from start to end), and `-1` specifies a step of -1, meaning it steps backward through the string.\n",
      "\n",
      "---\n",
      "\n",
      "### Method 2: Using `\"\".join()` and `reversed()`\n",
      "\n",
      "The `reversed()` function returns an iterator that yields items in reverse order. `\"\".join()` then concatenates these items back into a new string.\n",
      "\n",
      "```python\n",
      "def reverse_string_join_reversed(s):\n",
      "  \"\"\"Reverses a string using join() and reversed().\"\"\"\n",
      "  return \"\".join(reversed(s))\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"\\nJoin and Reversed Method:\")\n",
      "print(f\"'world' reversed is: {reverse_string_join_reversed('world')}\") # Output: dlrow\n",
      "print(f\"'coding' reversed is: {reverse_string_join_reversed('coding')}\") # Output: gnidoc\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "*   `reversed(s)`: Takes an iterable (like a string) and returns an iterator that yields elements in reverse order. For \"hello\", it would yield 'o', then 'l', then 'l', then 'e', then 'h'.\n",
      "*   `\"\".join(...)`: Concatenates the elements from the iterator into a single string, using an empty string as the separator.\n",
      "\n",
      "---\n",
      "\n",
      "### Method 3: Using a Loop (Building a new string)\n",
      "\n",
      "This method iterates through the original string and prepends each character to a new string, effectively reversing it.\n",
      "\n",
      "```python\n",
      "def reverse_string_loop(s):\n",
      "  \"\"\"Reverses a string using a for loop.\"\"\"\n",
      "  reversed_s = \"\"\n",
      "  for char in s:\n",
      "    reversed_s = char + reversed_s # Prepend the character\n",
      "  return reversed_s\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"\\nLoop Method (Prepend):\")\n",
      "print(f\"'example' reversed is: {reverse_string_loop('example')}\") # Output: elpmaxe\n",
      "print(f\"'test' reversed is: {reverse_string_loop('test')}\")     # Output: tset\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "*   We initialize `reversed_s` as an empty string.\n",
      "*   For each `char` in the input string `s`, we add `char` to the *beginning* of `reversed_s`.\n",
      "*   Because strings are immutable in Python, `char + reversed_s` actually creates a *new* string in each iteration, which can be less efficient for very long strings compared to the slicing method.\n",
      "\n",
      "---\n",
      "\n",
      "### Method 4: Convert to List, Reverse, Join\n",
      "\n",
      "This method converts the string to a list of characters (which is mutable), reverses the list in-place, and then joins the characters back into a string.\n",
      "\n",
      "```python\n",
      "def reverse_string_list(s):\n",
      "  \"\"\"Reverses a string by converting to list, reversing, and joining.\"\"\"\n",
      "  char_list = list(s)       # Convert string to a list of characters\n",
      "  char_list.reverse()       # Reverse the list IN-PLACE\n",
      "  return \"\".join(char_list) # Join the characters back into a string\n",
      "\n",
      "# --- Examples ---\n",
      "print(f\"\\nList Conversion Method:\")\n",
      "print(f\"'mutable' reversed is: {reverse_string_list('mutable')}\") # Output: elbatum\n",
      "print(f\"'abc' reversed is: {reverse_string_list('abc')}\")         # Output: cba\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "*   `list(s)`: Converts the string \"abc\" to `['a', 'b', 'c']`.\n",
      "*   `char_list.reverse()`: Modifies the list in place, so `['a', 'b', 'c']` becomes `['c', 'b', 'a']`.\n",
      "*   `\"\".join(char_list)`: Joins the elements of the list back into a single string \"cba\".\n",
      "\n",
      "---\n",
      "\n",
      "### Which method to choose?\n",
      "\n",
      "*   **`s[::-1]` (Slicing):** This is almost always the **best choice** for its conciseness, readability, and performance. It's the most Pythonic way.\n",
      "*   **`\"\".join(reversed(s))`:** A very close second in terms of Pythonic style and often used when dealing with iterators. Good for readability.\n",
      "*   **Loop-based methods (`reverse_string_loop`):** Good for understanding fundamental algorithms, but generally less efficient in Python for string reversal due to the overhead of creating new string objects in each iteration.\n",
      "*   **List conversion (`reverse_string_list`):** Also clear and demonstrates mutable list operations, but involves more steps than slicing or `join(reversed())`.\n",
      "\n",
      "**For most practical purposes, stick with `s[::-1]`.**\n"
     ]
    }
   ],
   "source": [
    "# Model Connectivity Test (Sample Prompt)\n",
    "\n",
    "# Send a test prompt to verify the Gemini model is working correctly\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemini-2.5-flash',\n",
    "    contents = 'Provide Python code to reverse a string.'\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc0debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe Generation Prompt Template\n",
    "\n",
    "RECIPE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Expert chef tasked with reducing food waste, specifically focused on home cooking and meal planning. Your goal is generate an ideal recipe based on available ingredients and a specific cuisine style.\n",
    "Examine the following user inventory and cuisine preference and determine the best possible recipe that utilizes the inventory efficiently.\n",
    "\n",
    "Here are the specific rules to follow when generating the recipe:\n",
    "1. You must prioritize the ingridients provided in the inventory.\n",
    "2. You may assume the user has basic pantry staples (salt,pepper,oil,water,basic spices).\n",
    "3. The recipe must strictly adhere to the requested cuisine type (eg. if indian, use indian spice/methods).\n",
    "4. Provide clear, step-by-step cooking instructions.\n",
    "\n",
    "Focus on etracting the following details:\n",
    "1. Recipe Name\n",
    "2. List of ingredients (with quantities)\n",
    "3.Step-by-step Instructions\n",
    "4. Cooking time\n",
    "\n",
    "Prioritise creating a cohesive, tasty dish over using every single random ingredient.\n",
    "do NOT alter, rephrase the user's inventory items into things they didn't list (unless they are basic staples).\n",
    "\n",
    "Exclude:\n",
    "1. ingredients that are rare or highly specific if they are not in the inventory.\n",
    "2. Extremely complex prefessional cooking techniques (keep it home-cook friendly)\n",
    "\n",
    "Output your response as a single line JSON string according to the following structure and nothing else:\n",
    "[\n",
    "  {{\n",
    "    'RecipeName':'Name of the dish',\n",
    "    'Ingredients':['List','of','Ingredients','and quantities'],\n",
    "    'Instructions':['Step 1...','Step 2...'],\n",
    "    'PrepTime':'XX mins',\n",
    "    'CuisineType':'The Cuisine Selected'\n",
    "    }}\n",
    "]\n",
    "\n",
    "[INPUT DATA LABEL]:\n",
    "Inventory:{inventory}\n",
    "Cuisine Preference: {cuisine}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e0e48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Input Declaration\n",
    "\n",
    "# Ingredients available with the user\n",
    "my_inventory = 'Paneer,onions,tomatoes,yogurt,garlic,ginger'\n",
    "\n",
    "# Preferred cuisine type\n",
    "desired_cuisine = 'Indian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf87cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Prompt Construction\n",
    "\n",
    "# Populate the prompt template with user-provided inputs\n",
    "final_prompt = RECIPE_PROMPT_TEMPLATE.format(\n",
    "    inventory = my_inventory,\n",
    "    cuisine = desired_cuisine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fba1e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/gemini-2.5-flash' display_name='Gemini 2.5 Flash' description='Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-pro' display_name='Gemini 2.5 Pro' description='Stable release (June 17th, 2025) of Gemini 2.5 Pro' version='2.5' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash-exp-image-generation' display_name='Gemini 2.0 Flash (Image Generation) Experimental' description='Gemini 2.0 Flash (Image Generation) Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=40 thinking=None\n",
      "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-flash-preview-tts' display_name='Gemini 2.5 Flash Preview TTS' description='Gemini 2.5 Flash Preview TTS' version='gemini-2.5-flash-exp-tts-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=16384 supported_actions=['countTokens', 'generateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemini-2.5-pro-preview-tts' display_name='Gemini 2.5 Pro Preview TTS' description='Gemini 2.5 Pro Preview TTS' version='gemini-2.5-pro-preview-tts-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=16384 supported_actions=['countTokens', 'generateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemma-3-1b-it' display_name='Gemma 3 1B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=None top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemma-3-4b-it' display_name='Gemma 3 4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=None top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemma-3-12b-it' display_name='Gemma 3 12B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=None top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemma-3-27b-it' display_name='Gemma 3 27B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=None top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemma-3n-e4b-it' display_name='Gemma 3n E4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=2048 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=None top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemma-3n-e2b-it' display_name='Gemma 3n E2B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=2048 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=None top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemini-flash-latest' display_name='Gemini Flash Latest' description='Latest release of Gemini Flash' version='Gemini Flash Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-flash-lite-latest' display_name='Gemini Flash-Lite Latest' description='Latest release of Gemini Flash-Lite' version='Gemini Flash-Lite Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-pro-latest' display_name='Gemini Pro Latest' description='Latest release of Gemini Pro' version='Gemini Pro Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-flash-lite' display_name='Gemini 2.5 Flash-Lite' description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-flash-image-preview' display_name='Nano Banana' description='Gemini 2.5 Flash Preview Image' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=32768 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=1.0 top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemini-2.5-flash-image' display_name='Nano Banana' description='Gemini 2.5 Flash Preview Image' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=32768 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=1.0 top_p=0.95 top_k=64 thinking=None\n",
      "name='models/gemini-2.5-flash-preview-09-2025' display_name='Gemini 2.5 Flash Preview Sep 2025' description='Gemini 2.5 Flash Preview Sep 2025' version='Gemini 2.5 Flash Preview 09-2025' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-flash-lite-preview-09-2025' display_name='Gemini 2.5 Flash-Lite Preview Sep 2025' description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite' version='2.5-preview-09-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-3-pro-preview' display_name='Gemini 3 Pro Preview' description='Gemini 3 Pro Preview' version='3-pro-preview-11-2025' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-3-pro-image-preview' display_name='Nano Banana Pro' description='Gemini 3 Pro Image Preview' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=32768 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=1.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/nano-banana-pro-preview' display_name='Nano Banana Pro' description='Gemini 3 Pro Image Preview' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=32768 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=1.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-robotics-er-1.5-preview' display_name='Gemini Robotics-ER 1.5 Preview' description='Gemini Robotics-ER 1.5 Preview' version='1.5-preview' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-computer-use-preview-10-2025' display_name='Gemini 2.5 Computer Use Preview 10-2025' description='Gemini 2.5 Computer Use Preview 10-2025' version='Gemini 2.5 Computer Use Preview 10-2025' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=65536 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/deep-research-pro-preview-12-2025' display_name='Deep Research Pro Preview (Dec-12-2025)' description='Preview release (December 12th, 2025) of Deep Research Pro' version='deepthink-exp-05-20' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=65536 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/gemini-embedding-exp-03-07' display_name='Gemini Embedding Experimental 03-07' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/gemini-embedding-exp' display_name='Gemini Embedding Experimental' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens', 'countTokens'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/gemini-embedding-001' display_name='Gemini Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer'] default_checkpoint_id=None checkpoints=None temperature=0.2 max_temperature=None top_p=1.0 top_k=40 thinking=None\n",
      "name='models/imagen-4.0-generate-preview-06-06' display_name='Imagen 4 (Preview)' description='Vertex served Imagen 4.0 model' version='01' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/imagen-4.0-ultra-generate-preview-06-06' display_name='Imagen 4 Ultra (Preview)' description='Vertex served Imagen 4.0 ultra model' version='01' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/imagen-4.0-generate-001' display_name='Imagen 4' description='Vertex served Imagen 4.0 model' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/imagen-4.0-ultra-generate-001' display_name='Imagen 4 Ultra' description='Vertex served Imagen 4.0 ultra model' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/imagen-4.0-fast-generate-001' display_name='Imagen 4 Fast' description='Vertex served Imagen 4.0 Fast model' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/veo-2.0-generate-001' display_name='Veo 2' description='Vertex served Veo 2 model. Access to this model requires billing to be enabled on the associated Google Cloud Platform account. Please visit https://console.cloud.google.com/billing to enable it.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/veo-3.0-generate-001' display_name='Veo 3' description='Veo 3' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/veo-3.0-fast-generate-001' display_name='Veo 3 fast' description='Veo 3 fast' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/veo-3.1-generate-preview' display_name='Veo 3.1' description='Veo 3.1' version='3.1' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/veo-3.1-fast-generate-preview' display_name='Veo 3.1 fast' description='Veo 3.1 fast' version='3.1' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None temperature=None max_temperature=None top_p=None top_k=None thinking=None\n",
      "name='models/gemini-2.5-flash-native-audio-latest' display_name='Gemini 2.5 Flash Native Audio Latest' description='Latest release of Gemini 2.5 Flash Native Audio' version='Gemini 2.5 Flash Native Audio Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-flash-native-audio-preview-09-2025' display_name='Gemini 2.5 Flash Native Audio Preview 09-2025' description='Gemini 2.5 Flash Native Audio Preview 09-2025' version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n",
      "name='models/gemini-2.5-flash-native-audio-preview-12-2025' display_name='Gemini 2.5 Flash Native Audio Preview 12-2025' description='Gemini 2.5 Flash Native Audio Preview 12-2025' version='12-2025' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None temperature=1.0 max_temperature=2.0 top_p=0.95 top_k=64 thinking=True\n"
     ]
    }
   ],
   "source": [
    "# List Available Gemini Models\n",
    "\n",
    "# Retrieve all available models linked to the API key\n",
    "models = client.models.list()\n",
    "\n",
    "# Display model details\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3b8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    'RecipeName': 'Spicy Paneer & Tomato Curry with Basmati Rice',\n",
      "    'Ingredients': ['Paneer', 'Onions', 'Tomatoes', 'Garlic', 'Ginger', 'Yogurt', 'Spices (Turmeric, Cumin, Coriander, Chili Powder, Garam Masala)', 'Basmati Rice', 'Salt', 'Oil'],\n",
      "    'Instructions': [\n",
      "      \"1. Dice the onions, garlic, and ginger into small pieces.\",\n",
      "      \"2. Heat oil in a pot over medium heat. Add the onions and sauté until translucent (about 5-7 minutes).\",\n",
      "      \"3. Add the garlic and ginger and sauté for another minute until fragrant.\",\n",
      "      \"4. Add the tomatoes and cook for 5-7 minutes, stirring occasionally, until they soften.\",\n",
      "      \"5. Gently add the paneer and cook for 3-5 minutes, allowing it to brown slightly.\",\n",
      "      \"6. Stir in the spices – turmeric, cumin, coriander, chili powder, and garam masala – and cook for 1 minute, allowing the spices to bloom.\",\n",
      "      \"7. Pour in the yogurt and bring to a simmer.  Add salt to taste.\",\n",
      "      \"8. Simmer for 10-15 minutes, allowing the flavors to meld.\",\n",
      "      \"9. Serve hot with basmati rice.\"\n",
      "    ],\n",
      "    'PrepTime': 20,\n",
      "    'CuisineType': 'Indian'\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Generate Recipe Response from Gemini\n",
    "\n",
    "# Generate a response using the Gemini text model and user inputs\n",
    "response = client.models.generate_content(\n",
    "    model = 'gemma-3-1b-it',\n",
    "    contents = final_prompt\n",
    ")\n",
    "\n",
    "# Display the generated recipe\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e46c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Spicy Paneer & Tomato Curry with Basmati Rice',\n",
       " ['Paneer',\n",
       "  'Onions',\n",
       "  'Tomatoes',\n",
       "  'Garlic',\n",
       "  'Ginger',\n",
       "  'Yogurt',\n",
       "  'Spices (Turmeric, Cumin, Coriander, Chili Powder, Garam Masala)',\n",
       "  'Basmati Rice',\n",
       "  'Salt',\n",
       "  'Oil'],\n",
       " ['1. Dice the onions, garlic, and ginger into small pieces.',\n",
       "  '2. Heat oil in a pot over medium heat. Add the onions and sauté until translucent (about 5-7 minutes).',\n",
       "  '3. Add the garlic and ginger and sauté for another minute until fragrant.',\n",
       "  '4. Add the tomatoes and cook for 5-7 minutes, stirring occasionally, until they soften.',\n",
       "  '5. Gently add the paneer and cook for 3-5 minutes, allowing it to brown slightly.',\n",
       "  '6. Stir in the spices – turmeric, cumin, coriander, chili powder, and garam masala – and cook for 1 minute, allowing the spices to bloom.',\n",
       "  '7. Pour in the yogurt and bring to a simmer.  Add salt to taste.',\n",
       "  '8. Simmer for 10-15 minutes, allowing the flavors to meld.',\n",
       "  '9. Serve hot with basmati rice.'],\n",
       " 20,\n",
       " 'Indian')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Response Cleaning & Parsing\n",
    "\n",
    "def clean_text(response):\n",
    "    raw_text = response.text\n",
    "\n",
    "     # Validate response content\n",
    "    if not raw_text or not raw_text.strip():\n",
    "        raise ValueError('Empty response from model')\n",
    "    \n",
    "    # Normalize and clean text\n",
    "    text = raw_text.strip()\n",
    "    text = re.sub(r\"'''(?:json)?\",\"\",text).strip()\n",
    "\n",
    "    # Extract JSON-like list from the response\n",
    "    match = re.search(r\"(\\[.*\\])\",text,re.DOTALL)\n",
    "\n",
    "    if not match:\n",
    "        raise ValueError('No recipe data found')\n",
    "    \n",
    "    # Safely parse the extracted data\n",
    "    data = ast.literal_eval(match.group(1))\n",
    "    recipe_data = data[0]\n",
    "\n",
    "    return (recipe_data['RecipeName'],\n",
    "            recipe_data['Ingredients'],\n",
    "            recipe_data['Instructions'],\n",
    "            recipe_data['PrepTime'],\n",
    "            recipe_data['CuisineType']\n",
    "    )\n",
    "\n",
    "clean_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba2aa98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RECIPE FOUND: Spicy Paneer & Tomato Curry with Basmati Rice\n",
      " Time: 20\n",
      " Cuisine: Indian\n",
      "________________________________________\n",
      " INGREDIENTS:\n",
      "- Paneer\n",
      "- Onions\n",
      "- Tomatoes\n",
      "- Garlic\n",
      "- Ginger\n",
      "- Yogurt\n",
      "- Spices (Turmeric, Cumin, Coriander, Chili Powder, Garam Masala)\n",
      "- Basmati Rice\n",
      "- Salt\n",
      "- Oil\n",
      "----------------------------------------\n",
      " INSTRUCTIONS:\n",
      "1. Dice the onions, garlic, and ginger into small pieces.\n",
      "2. Heat oil in a pot over medium heat. Add the onions and sauté until translucent (about 5-7 minutes).\n",
      "3. Add the garlic and ginger and sauté for another minute until fragrant.\n",
      "4. Add the tomatoes and cook for 5-7 minutes, stirring occasionally, until they soften.\n",
      "5. Gently add the paneer and cook for 3-5 minutes, allowing it to brown slightly.\n",
      "6. Stir in the spices – turmeric, cumin, coriander, chili powder, and garam masala – and cook for 1 minute, allowing the spices to bloom.\n",
      "7. Pour in the yogurt and bring to a simmer.  Add salt to taste.\n",
      "8. Simmer for 10-15 minutes, allowing the flavors to meld.\n",
      "9. Serve hot with basmati rice.\n"
     ]
    }
   ],
   "source": [
    "# Final Recipe Output Presentation\n",
    "\n",
    "def final_answer(response):\n",
    "    recipeName, ingredients, instructions, prepTime, cuisineType = clean_text(response)\n",
    "\n",
    "    print(f' RECIPE FOUND: {recipeName}')\n",
    "    print(f' Time: {prepTime}')\n",
    "    print(f' Cuisine: {cuisineType}')\n",
    "    print('_'* 40)\n",
    "    print(' INGREDIENTS:')\n",
    "    for item in ingredients:\n",
    "        print(f'- {item}')\n",
    "        \n",
    "    print('-' * 40)\n",
    "    print(' INSTRUCTIONS:')\n",
    "    for step in instructions:\n",
    "        print(f'{step}')   \n",
    "\n",
    "final_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37a4cfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RECIPE FOUND: Spicy Paneer & Tomato Curry with Basmati Rice\n",
      " Time: 20\n",
      " Cuisine: Indian\n",
      "________________________________________\n",
      " INGREDIENTS:\n",
      "- Paneer\n",
      "- Onions\n",
      "- Tomatoes\n",
      "- Garlic\n",
      "- Ginger\n",
      "- Yogurt\n",
      "- Spices (Turmeric, Cumin, Coriander, Chili Powder, Garam Masala)\n",
      "- Basmati Rice\n",
      "- Salt\n",
      "- Oil\n",
      "----------------------------------------\n",
      " INSTRUCTIONS:\n",
      "1. Dice the onions, garlic, and ginger into small pieces.\n",
      "2. Heat oil in a pot over medium heat. Add the onions and sauté until translucent (about 5-7 minutes).\n",
      "3. Add the garlic and ginger and sauté for another minute until fragrant.\n",
      "4. Add the tomatoes and cook for 5-7 minutes, stirring occasionally, until they soften.\n",
      "5. Gently add the paneer and cook for 3-5 minutes, allowing it to brown slightly.\n",
      "6. Stir in the spices – turmeric, cumin, coriander, chili powder, and garam masala – and cook for 1 minute, allowing the spices to bloom.\n",
      "7. Pour in the yogurt and bring to a simmer.  Add salt to taste.\n",
      "8. Simmer for 10-15 minutes, allowing the flavors to meld.\n",
      "9. Serve hot with basmati rice.\n"
     ]
    }
   ],
   "source": [
    "final_answer(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2231aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youar\\AppData\\Local\\Temp\\ipykernel_36724\\3874923906.py:147: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://e5978b933a07f3f04f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e5978b933a07f3f04f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment Configuration\n",
    "\n",
    "# Password stored securely in environment variables\n",
    "Password = os.getenv('PASSWORD')\n",
    "\n",
    "# Authentication Logic\n",
    "def verify_answer(pswd):\n",
    "    if pswd == Password:\n",
    "        return (\n",
    "            'Access Granted !',\n",
    "            gr.update(visible=True),\n",
    "            gr.update(visible=True),\n",
    "            gr.update(visible=True),\n",
    "            gr.update(visible=True),\n",
    "            gr.update(visible=False,value=''),\n",
    "        )\n",
    "    else:\n",
    "        return(\n",
    "            'Invalid credentials ! Please login with correct credentials',\n",
    "            gr.update(visible=False),\n",
    "            gr.update(visible=False),\n",
    "            gr.update(visible=False),\n",
    "            gr.update(visible=False),\n",
    "            gr.update(visible=False,value=''),\n",
    "        )\n",
    "\n",
    "# Prompt Template\n",
    "RECIPE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an Expert chef tasked with reducing food waste, specifically focused on home cooking and meal planning. Your goal is generate an ideal recipe based on available ingredients and a specific cuisine style.\n",
    "Examine the following user inventory and cuisine preference and determine the best possible recipe that utilizes the inventory efficiently.\n",
    "\n",
    "Here are the specific rules to follow when generating the recipe:\n",
    "1. You must prioritize the ingridients provided in the inventory.\n",
    "2. You may assume the user has basic pantry staples (salt,pepper,oil,water,basic spices).\n",
    "3. The recipe must strictly adhere to the requested cuisine type (eg. if indian, use indian spice/methods).\n",
    "4. Provide clear, step-by-step cooking instructions.\n",
    "\n",
    "Focus on etracting the following details:\n",
    "1. Recipe Name\n",
    "2. List of ingredients (with quantities)\n",
    "3.Step-by-step Instructions\n",
    "4. Cooking time\n",
    "\n",
    "Prioritise creating a cohesive, tasty dish over using every single random ingredient.\n",
    "do NOT alter, rephrase the user's inventory items into things they didn't list (unless they are basic staples).\n",
    "\n",
    "Exclude:\n",
    "1. ingredients that are rare or highly specific if they are not in the inventory.\n",
    "2. Extremely complex prefessional cooking techniques (keep it home-cook friendly)\n",
    "\n",
    "Output your response as a single line JSON string according to the following structure and nothing else:\n",
    "[\n",
    "  {{\n",
    "    'RecipeName':'Name of the dish',\n",
    "    'Ingredients':['List','of','Ingredients','and quantities'],\n",
    "    'Instructions':['Step 1...','Step 2...'],\n",
    "    'PrepTime':'XX mins',\n",
    "    'CuisineType':'The Cuisine Selected'\n",
    "    }}\n",
    "]\n",
    "\n",
    "[INPUT DATA LABEL]:\n",
    "Inventory:{inventory}\n",
    "Cuisine Preference: {cuisine}\n",
    "\"\"\"\n",
    "\n",
    "# Model Response Cleaning & Parsing\n",
    "def clean_text(response):\n",
    "    raw_text = response.text\n",
    "\n",
    "    if not raw_text or not raw_text.strip():\n",
    "        raise ValueError('Empty response from model')\n",
    "    \n",
    "    text = raw_text.strip()\n",
    "    \n",
    "    # Remove markdown formatting if present\n",
    "    if \"```\" in text:\n",
    "        text = text.replace(\"```json\",\"\").replace(\"```\",\"\").strip()\n",
    "\n",
    "    # Extract JSON array\n",
    "    start = text.find(\"[\")\n",
    "    end = text.rfind(\"]\")\n",
    "\n",
    "    if start == -1 or end == -1:\n",
    "        raise ValueError(\"JSON array not found in model output\")\n",
    "    \n",
    "    json_text = text[start:end+1]\n",
    "    json_text= json_text.replace(\"'\",'\"')\n",
    "\n",
    "    try:\n",
    "        data = json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        data = ast.literal_eval(json_text)\n",
    "\n",
    "    recipe_data = data[0]\n",
    " \n",
    "    return (recipe_data[\"RecipeName\"],\n",
    "            recipe_data[\"Ingredients\"],\n",
    "            recipe_data[\"Instructions\"],\n",
    "            recipe_data[\"PrepTime\"],\n",
    "            recipe_data[\"CuisineType\"]\n",
    "    )\n",
    "\n",
    "# Model Execution Logic\n",
    "def model_run(Ingredients,desired_cuisine):\n",
    "\n",
    "    if not Ingredients or not desired_cuisine:\n",
    "        return gr.update(value='Please enter both ingredients and cuisine.',visible=True)\n",
    "    \n",
    "    final_prompt = RECIPE_PROMPT_TEMPLATE.format(\n",
    "    inventory = Ingredients,\n",
    "    cuisine = desired_cuisine\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model = 'gemma-3-1b-it',\n",
    "            contents = final_prompt\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return gr.update(value=f'Model Error: {str(e)}', visible=True) \n",
    "        \n",
    "    try:\n",
    "        name,ing,steps,time,ctype = clean_text(response)\n",
    "    except Exception as e:\n",
    "        return gr.update(value=f\"Parsing error: {str(e)}\\n\\nRaw response: {response.text[:500]}\",visible=True)\n",
    "    \n",
    "    output = f\"\"\"\n",
    "RECIPE: {name}\n",
    "Time: {time}\n",
    "Cuisine: {ctype}\n",
    "\n",
    "    Ingredients:\n",
    "    \"\"\" + '\\n'.join(f'- {i}' for i in ing) + \"\\n\\nINSTRUCTIONS:\\n\" + \"\\n\".join(f' {i+1}. {step}' for i,step in enumerate(steps))\n",
    "\n",
    "    return gr.update(value=output.strip(),visible=True)\n",
    "\n",
    "# Clear Inputs\n",
    "def clear_all():\n",
    "    return (\n",
    "        gr.update(value='',visible=True),\n",
    "        gr.update(value='',visible=True),\n",
    "        gr.update(value='',visible=False)\n",
    "    )\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "        <div align=\"center\">\n",
    "          <h1 style=\"font-size: 42px; color: #2c3e50;\">\n",
    "                    Gemini Recipe Generator\n",
    "          </h1>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    \n",
    "    password = gr.Textbox(label = 'Password',type='password',placeholder='Enter password to access')\n",
    "    auth_message = gr.Textbox(label=\"Status\", interactive=False)\n",
    "    Ingredients = gr.Textbox(label = 'Enter Ingredients (Seperated by Coma)',visible=False,placeholder='eg....Paneer,onions,tomatoes,yogurt,garlic,ginger')\n",
    "    desired_cuisine = gr.Textbox(label = 'Enter Cuisine Style',visible=False,placeholder='eg...Indian,Italian,Chinese,Mexican')\n",
    "    submit = gr.Button('Generate Recipe',visible=False,variant='primary')\n",
    "    clear = gr.Button('New Recipe',visible=False)\n",
    "    Response = gr.Textbox(label = 'Your Recipe is Ready',visible=False)\n",
    "\n",
    "    password.submit(verify_answer,\n",
    "                    inputs=password,\n",
    "                    outputs=[auth_message,Ingredients,desired_cuisine,submit,clear,Response])\n",
    "\n",
    "    submit.click(model_run,\n",
    "                inputs=[Ingredients,desired_cuisine],\n",
    "                outputs=Response)\n",
    "    \n",
    "    clear.click(\n",
    "        clear_all,\n",
    "        inputs=None,\n",
    "        outputs=[Ingredients,desired_cuisine,Response]\n",
    "    )\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
